{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0f0ec4",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Gen AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a9e453b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "322402ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAI_API_KEY'] = os.getenv(\"LANGCHAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ['LANGCHAI_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANCHAIN_TRACING_V2\"]=\"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea3ed69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='models/gemini-2.5-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000018AEACE0D10> default_metadata=() model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\"\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e62e4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "026e6e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Generative AI** refers to a category of artificial intelligence models that are designed to **create new, original content** rather than just analyze or classify existing data.\n",
      "\n",
      "Unlike traditional AI that might identify a cat in an image (classification) or recommend a product based on past purchases (prediction), generative AI can produce:\n",
      "\n",
      "*   **Text:** Articles, stories, poems, code, summaries, conversations (e.g., ChatGPT, Gemini).\n",
      "*   **Images:** Photorealistic pictures, artistic illustrations, design elements (e.g., DALL-E, Midjourney, Stable Diffusion).\n",
      "*   **Audio:** Music, speech, sound effects.\n",
      "*   **Video:** Short clips, animated sequences.\n",
      "*   **Code:** Programming snippets, functions, entire scripts.\n",
      "*   **3D Models:** Objects for games, simulations, or design.\n",
      "\n",
      "### How Generative AI Works (Simplified):\n",
      "\n",
      "1.  **Training Data:** Generative AI models are trained on vast datasets of existing content (e.g., millions of images, billions of text documents, hours of audio).\n",
      "2.  **Pattern Recognition:** During training, the model learns the underlying patterns, structures, styles, and relationships within this data. It doesn't just memorize; it learns *how* things are put together. For example, a text model learns grammar, syntax, tone, and factual relationships, while an image model learns shapes, colors, textures, and common object arrangements.\n",
      "3.  **Generation:** When given a prompt or input (e.g., \"Write a short story about a robot who loves to paint,\" or \"Create an image of an astronaut riding a horse on the moon in a watercolor style\"), the model uses its learned knowledge to predict and assemble new content that aligns with the prompt and the patterns it has observed. It doesn't copy existing content; it synthesizes novel outputs.\n",
      "\n",
      "### Key Characteristics:\n",
      "\n",
      "*   **Originality:** It creates content that did not exist before.\n",
      "*   **Versatility:** Can generate content across various modalities (text, image, audio, etc.).\n",
      "*   **Contextual Understanding:** Can often understand and respond to nuanced or complex prompts.\n",
      "*   **Creativity (Simulated):** While not truly \"creative\" in the human sense, it can produce outputs that appear highly imaginative and novel.\n",
      "\n",
      "### Examples of Generative AI in Action:\n",
      "\n",
      "*   **Large Language Models (LLMs):** Like OpenAI's ChatGPT, Google's Gemini, or Anthropic's Claude, which can generate human-like text for various purposes.\n",
      "*   **Image Generators:** Like Midjourney, DALL-E, or Stable Diffusion, which create images from text descriptions.\n",
      "*   **Music Generators:** AI tools that compose original musical pieces in various styles.\n",
      "*   **Code Generators:** AI assistants that write or complete programming code.\n",
      "\n",
      "Generative AI is a rapidly evolving field with the potential to revolutionize many industries, from creative arts and entertainment to software development and scientific research.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea648889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "372cf8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a powerful platform developed by LangChain for **observability, debugging, testing, and monitoring of Large Language Model (LLM) applications**.\n",
      "\n",
      "Think of it as an Application Performance Management (APM) tool specifically designed for the unique challenges of building and maintaining applications powered by LLMs.\n",
      "\n",
      "Here's a breakdown of what LangSmith is and why it's crucial for LLM development:\n",
      "\n",
      "## What Problem Does LangSmith Solve?\n",
      "\n",
      "Developing reliable and performant LLM applications is challenging for several reasons:\n",
      "\n",
      "1.  **Non-Determinism:** LLMs are inherently non-deterministic. The same prompt might yield slightly different results, making debugging difficult.\n",
      "2.  **Black Box Nature:** It's hard to understand *why* an LLM or an entire chain of LLM calls produced a particular output. Where did it go wrong? What was the intermediate thought process?\n",
      "3.  **Complexity of Chains/Agents:** Modern LLM applications often involve complex chains, agents, tool usage, and retrievers. Tracking the flow of data and decisions across these components is nearly impossible without specialized tools.\n",
      "4.  **Evaluation & Testing:** Traditional unit tests don't fit well for LLM outputs. How do you objectively test the \"quality\" or \"correctness\" of a generated response?\n",
      "5.  **Production Monitoring:** Once deployed, how do you track performance, cost, latency, and identify regressions or unexpected behaviors?\n",
      "6.  **Iteration & Experimentation:** How do you systematically compare different prompts, models, or chain configurations to find the best performing one?\n",
      "\n",
      "LangSmith addresses these pain points by providing an end-to-end platform for the entire LLM application lifecycle.\n",
      "\n",
      "## Key Features and Capabilities\n",
      "\n",
      "LangSmith offers a suite of features designed to streamline LLM development:\n",
      "\n",
      "1.  **Tracing & Debugging:**\n",
      "    *   **Visualizing Call Chains:** It provides a detailed, step-by-step visualization of every component execution within a LangChain (or other framework) run. You can see inputs, outputs, intermediate steps (like an agent's thought process), and errors for each LLM call, tool invocation, or chain step.\n",
      "    *   **Input/Output Inspection:** Easily inspect the exact prompts sent to LLMs and the responses received.\n",
      "    *   **Latency & Token Usage:** Track the latency and token usage for each step, helping identify bottlenecks and optimize costs.\n",
      "    *   **Error Identification:** Quickly pinpoint where an error occurred within a complex chain.\n",
      "\n",
      "2.  **Testing & Evaluation:**\n",
      "    *   **Dataset Management:** Create and manage datasets of test cases (input/expected output pairs) to evaluate your LLM application.\n",
      "    *   **Automated Evaluators:** Run automated evaluations against your datasets using various metrics (e.g., correctness, coherence, toxicity, factual consistency) or even using an LLM to evaluate the output.\n",
      "    *   **Human Annotation & Feedback:** Collect human feedback on runs, allowing you to refine your models and prompts based on real-world performance.\n",
      "    *   **Regression Testing:** Ensure that changes to your prompts, models, or chain logic don't introduce regressions.\n",
      "\n",
      "3.  **Experimentation & Comparison:**\n",
      "    *   **A/B Testing:** Compare different versions of your application (e.g., different prompts, different LLMs, different chain configurations) side-by-side to determine which performs better.\n",
      "    *   **Run Comparison:** Easily compare traces and evaluation results of different runs or experiments.\n",
      "\n",
      "4.  **Monitoring & Production Analytics:**\n",
      "    *   **Dashboard & Metrics:** Get high-level dashboards showing key metrics like average latency, token usage, cost, and error rates over time.\n",
      "    *   **Feedback Loops:** Integrate user feedback directly into the platform to continuously improve your application.\n",
      "    *   **Data Collection for Fine-tuning:** The recorded traces can serve as valuable data for fine-tuning custom LLMs or improving retrieval systems.\n",
      "\n",
      "5.  **Collaboration:**\n",
      "    *   **Shared Workspaces:** Teams can collaborate on projects, sharing runs, datasets, and evaluations.\n",
      "\n",
      "## How it Integrates with LangChain (and Beyond)\n",
      "\n",
      "LangSmith is developed by the creators of LangChain, so it has deep and seamless integration. When you use LangChain, enabling LangSmith tracing is as simple as setting a few environment variables (`LANGCHAIN_TRACING_V2=true`, `LANGCHAIN_API_KEY`, `LANGCHAIN_PROJECT`).\n",
      "\n",
      "However, it's important to note that **LangSmith is not exclusively for LangChain users**. While LangChain provides out-of-the-box instrumentation, you can also manually instrument your code using LangSmith's client libraries to trace applications built with other frameworks (like LlamaIndex, LiteLLM, or even raw OpenAI API calls).\n",
      "\n",
      "## Who is LangSmith For?\n",
      "\n",
      "*   **AI/ML Engineers:** For building, debugging, and optimizing LLM-powered applications.\n",
      "*   **Data Scientists:** For experimenting with different models and prompts, and evaluating their performance.\n",
      "*   **Developers:** Anyone building applications that incorporate LLMs and needs better visibility into their behavior.\n",
      "*   **Teams:** For collaborating on LLM projects and ensuring quality throughout the development lifecycle.\n",
      "\n",
      "## In Summary\n",
      "\n",
      "LangSmith is an **essential tool for anyone serious about building robust, reliable, and performant LLM applications**. It provides the necessary observability, testing, and monitoring capabilities that are standard in traditional software development but have been largely missing for the unique challenges of generative AI. By using LangSmith, developers can iterate faster, debug more effectively, and deploy with greater confidence.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a88ebf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba65c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a developer platform specifically designed to help engineers build, debug, test, and monitor **Large Language Model (LLM) applications**. It's developed by LangChain Inc., the creators of the popular LangChain framework, and is an integral part of the broader LangChain ecosystem.\n",
      "\n",
      "Think of LangSmith as the \"observability and MLOps platform\" for your LLM applications, much like Datadog or Sentry are for traditional software applications, but tailored for the unique challenges of LLMs.\n",
      "\n",
      "### Why was LangSmith created? (The Problem it Solves)\n",
      "\n",
      "Building robust and reliable LLM applications, especially those using complex chains or agents (like in LangChain), is challenging for several reasons:\n",
      "\n",
      "1.  **Non-Determinism:** LLMs are inherently probabilistic. The same prompt might yield slightly different results, making debugging difficult.\n",
      "2.  **Black Box Nature:** It's hard to understand *why* an LLM responded a certain way or *where* a chain went wrong.\n",
      "3.  **Complexity of Chains:** Multi-step chains involving multiple LLM calls, tool uses, retrievals (RAG), and conditional logic are opaque without proper tracing.\n",
      "4.  **Debugging Pain:** Traditional debuggers don't easily visualize the flow of LLM interactions, token usage, or intermediate steps.\n",
      "5.  **Evaluation:** How do you systematically test if your LLM app is performing correctly, especially as models or prompts change?\n",
      "6.  **Production Monitoring:** How do you track performance, cost, latency, and identify issues in live LLM applications?\n",
      "\n",
      "LangSmith addresses these pain points by providing visibility and tools throughout the LLM application lifecycle.\n",
      "\n",
      "### Key Features and Components\n",
      "\n",
      "LangSmith offers a suite of features that can be broadly categorized into:\n",
      "\n",
      "1.  **Tracing & Debugging:**\n",
      "    *   **Visualizing Runs:** It provides a detailed, hierarchical trace of every step in your LangChain (or even custom LLM) application. You can see each LLM call, tool invocation, retrieval step, and the inputs/outputs at each stage.\n",
      "    *   **Detailed Inspection:** For each step, you can inspect the exact prompts sent to the LLM, the raw responses received, the parameters used (temperature, max tokens), and even the token counts and latency.\n",
      "    *   **Error Identification:** Quickly pinpoint where an error occurred in a complex chain, rather than just seeing a generic error message.\n",
      "    *   **Comparing Runs:** Easily compare different runs of your application (e.g., before and after a code change) to understand performance differences or regressions.\n",
      "    *   **\"Like Chrome DevTools for your LLM App\":** This is a common analogy, as it allows you to \"inspect\" the internal workings.\n",
      "\n",
      "2.  **Testing & Evaluation:**\n",
      "    *   **Datasets:** Create and manage datasets of input-output pairs that represent desired behaviors or test cases for your application.\n",
      "    *   **Automated Evaluators:** Run your chains against these datasets and use built-in or custom evaluators (e.g., correctness, coherence, toxicity, faithfulness for RAG) to automatically score the outputs.\n",
      "    *   **Human Feedback:** Integrate human feedback into your evaluation process, allowing annotators to rate the quality of responses and provide corrections.\n",
      "    *   **A/B Testing:** Compare different versions of your chains, prompts, or models against the same datasets to determine which performs better.\n",
      "    *   **Regression Testing:** Ensure that new changes don't negatively impact existing functionality.\n",
      "\n",
      "3.  **Monitoring & Observability:**\n",
      "    *   **Production Monitoring:** Track key metrics for your live applications, such as latency, token usage, cost, and error rates.\n",
      "    *   **Feedback Collection:** Integrate user feedback mechanisms directly into your application, sending data to LangSmith for analysis and improvement.\n",
      "    *   **Identifying Drift:** Monitor how your application's performance changes over time as user queries or underlying models evolve.\n",
      "\n",
      "4.  **Prompt & Model Management (Implicit):** While not a direct prompt hub, the ability to trace, test, and compare different prompt versions effectively provides a robust prompt engineering workflow.\n",
      "\n",
      "### How it Integrates\n",
      "\n",
      "*   **LangChain Integration:** LangSmith is deeply integrated with the LangChain framework. By simply setting a few environment variables (`LANGCHAIN_TRACING_V2=true`, `LANGCHAIN_API_KEY`, `LANGCHAIN_PROJECT`), your LangChain applications will automatically send their execution traces to LangSmith.\n",
      "*   **Standalone Client:** You can also use the `langsmith` Python client library to trace any Python code, even if it's not using LangChain. This makes it versatile for custom LLM orchestration logic.\n",
      "*   **API/UI:** All data is accessible via a web-based UI and a robust API for programmatic access and integration with other systems.\n",
      "\n",
      "### Benefits of Using LangSmith\n",
      "\n",
      "*   **Faster Debugging:** Quickly identify and fix issues in complex LLM chains.\n",
      "*   **Improved Reliability:** Systematically test and validate your applications, reducing unexpected behavior.\n",
      "*   **Better Performance:** Optimize for cost and latency by identifying bottlenecks.\n",
      "*   **Data-Driven Development:** Make informed decisions based on real usage and evaluation metrics.\n",
      "*   **Enhanced Collaboration:** Share traces and evaluation results with team members.\n",
      "*   **Confidence in Deployment:** Deploy LLM applications to production with greater assurance.\n",
      "*   **Accelerated Iteration:** Rapidly experiment with new prompts, models, and architectures.\n",
      "\n",
      "### Who is it for?\n",
      "\n",
      "*   **LLM Engineers/Developers:** Essential for anyone building LLM-powered applications, especially with LangChain.\n",
      "*   **MLOps Engineers:** For monitoring and managing LLM applications in production.\n",
      "*   **Product Managers:** To understand application performance and user experience.\n",
      "*   **Researchers:** For experimenting with and evaluating different LLM approaches.\n",
      "\n",
      "In essence, LangSmith transforms the often chaotic and opaque process of building LLM applications into a more structured, observable, and systematically testable engineering discipline. It's becoming an indispensable tool for anyone serious about developing robust, production-ready LLM systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parsor = StrOutputParser()\n",
    "chain = prompt|llm|output_parsor\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
